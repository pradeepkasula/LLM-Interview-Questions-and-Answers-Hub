# üöÄ LLM Interview Questions and Answers Hub
This repository includes 100+ LLM interview questions with answers.
![AIxFunda Newsletter](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/images/2-.jpg)


## Related Repositories
- üöÄ[Prompt Engineering Techniques Hub](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub)  - 25+ prompt engineering techniques with LangChain implementations.
- üë®üèª‚Äçüíª [LLM Engineer Toolkit](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) - Categories wise collection of 120+ LLM, RAG and Agent related libraries. 
- ü©∏[LLM, RAG and Agents Survey Papers Collection](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection) - Category wise collection of 200+ survey papers.

| # | Question | Answer |
|---|---------|--------|
| Q1 | CNNs and RNNs don‚Äôt use positional embeddings. Why do transformers use positional embeddings? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_1-3.md) |
| Q2 | Tell me the basic steps involved in running an inference query on an LLM. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_1-3.md) |
| Q3 | Explain how KV Cache accelerates LLM inference. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_1-3.md) |
| Q4 | How does quantization affect inference speed and memory requirements? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_4-6.md) |
| Q5 | How do you handle the large memory requirements of KV cache in LLM inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_4-6.md) |
| Q6 | After tokenization, how are tokens converted into embeddings in the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_4-6.md) |
| Q7 | Explain why subword tokenization is preferred over word-level tokenization in the Transformer model. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_7-9.md) |
| Q8 | Explain the trade-offs in using a large vocabulary in LLMs. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_7-9.md) |
| Q9 | Explain how self-attention is computed in the Transformer model step by step. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_7-9.md) |
| Q10 | What is the computational complexity of self-attention in the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_10-12.md) |
| Q11 | How do Transformer models address the vanishing gradient problem? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_10-12.md) |
| Q12 | What is tokenization, and why is it necessary in LLMs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_10-12.md) |
| Q13 | Explain the role of token embeddings in the Transformer model. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_13-15.md) |
| Q14 | Explain the working of the embedding layer in the Transformer model. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_13-15.md) |
| Q15 | What is the role of self-attention in the Transformer model, and why is it called ‚Äúself-attention‚Äù? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_13-15.md) |
| Q16 | What is the purpose of the encoder in a Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_16-18.md) |
| Q17 | What is the purpose of the decoder in a Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_16-18.md) |
| Q18 | How does the encoder-decoder structure work at a high level in the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_16-18.md) |
| Q19 | What is the purpose of scaling in the self-attention mechanism in the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_19-21.md) |
| Q20 | Why does the Transformer model use multiple self-attention heads instead of a single self-attention head? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_19-21.md) |
| Q21 | How are the outputs of multiple heads combined and projected back in the multi-head attention in the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_19-21.md) |
| Q22 | How does masked self-attention differ from regular self-attention, and where is it used in a Transformer? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_22-24.md) |
| Q23 | Discuss the pros and cons of the self-attention mechanism in the Transformer model. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_22-24.md) |
| Q24 | What is the purpose of masked self-attention in the Transformer decoder? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_22-24.md) |
| Q25 | Explain how masking works in masked self-attention in Transformer. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_25-27.md) |
| Q26 | Explain why self-attention in the decoder is referred to as cross-attention. How does it differ from self-attention in the encoder? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_25-27.md) |
| Q27 | What is the softmax function, and where is it applied in Transformers? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_25-27.md) |
| Q28 | What is the purpose of residual (skip) connections in Transformer layers? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_28-30.md) |
| Q29 | Why is layer normalization used, and where is it applied in Transformers? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_28-30.md) |
| Q30 | What is cross-entropy loss, and how is it applied during Transformer training? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_28-30.md) |
| Q31 | Compare Transformers and RNNs in terms of handling long-range dependencies. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_31-33.md) |
| Q32 | What are the fundamental limitations of the Transformer model? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_31-33.md) |
| Q33 | How do Transformers address the limitations of CNNs and RNNs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_31-33.md) |
| Q34 | How do Transformer models address the vanishing gradient problem? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_34-36.md) |
| Q35 | What is the purpose of the position-wise feed-forward sublayer? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_34-36.md) |
| Q36 | Can you briefly explain the difference between LLM training and inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_34-36.md) |
| Q37 | What is latency in LLM inference, and why is it important? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_37-39.md) |
| Q38 | What is batch inference, and how does it differ from single-query inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_37-39.md) |
| Q39 | How does batching generally help with LLM inference efficiency? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_37-39.md) |
| Q40 | Explain the trade-offs between batching and latency in LLM serving. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_40-42.md) |
| Q41 | How can techniques like mixture-of-experts (MoE) optimize inference efficiency? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_40-42.md) |
| Q42 | Explain the role of decoding strategy in LLM text generation. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_40-42.md) |
| Q43 | What are the different decoding strategies in LLMs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_43-45.md) |
| Q44 | Explain the impact of the decoding strategy on LLM-generated output quality and latency. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_43-45.md) |
| Q45 | Explain the greedy search decoding strategy and its main drawback. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_43-45.md) |
| Q46 | How does Beam Search improve upon Greedy Search, and what is the role of the beam width parameter? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_46-48.md) |
| Q47 | When is a deterministic strategy (like Beam Search) preferable to a stochastic (sampling) strategy? Provide a specific use case. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_46-48.md) |
| Q48 | Discuss the primary trade-off between the computational cost and the output quality when comparing Greedy Search and Beam Search. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_46-48.md) |
| Q49 | When you set the temperature to 0.0, which decoding strategy are you using? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_49-51.md) |
| Q50 | How is Beam Search fundamentally different from a Breadth-First Search (BFS) or Depth-First Search (DFS)? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_49-51.md) |
| Q51 | Explain the criteria for choosing different decoding strategies. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_49-51.md) |
| Q52 | Compare deterministic and stochastic decoding methods in LLMs. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_52-54.md) |
| Q53 | What is the role of the context window during LLM inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_52-54.md) |
| Q54 | Explain the pros and cons of large and small context windows in LLM inference. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_52-54.md) |
| Q55 | What is the purpose of temperature in LLM inference, and how does it affect the output? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_55-57.md) |
| Q56 | What is autoregressive generation in the context of LLMs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_55-57.md) |
| Q57 | Explain the strengths and limitations of autoregressive text generation in LLMs. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_55-57.md) |
| Q58 | Explain how diffusion language models (DLMs) differ from Large Language Models (LLMs). | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_58-60.md) |
| Q59 | Do you prefer DLMs or LLMs for latency-sensitive applications? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_58-60.md) |
| Q60 | Explain the concept of token streaming during inference. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_58-60.md) |
| Q61 | What is speculative decoding, and when would you use it? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_61-63.md) |
| Q62 | What are the challenges in performing distributed inference across multiple GPUs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_61-63.md) |
| Q63 | How would you design a scalable LLM inference system for real-time applications? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_61-63.md) |
| Q64 | Explain the role of Flash Attention in reducing memory bottlenecks. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_64-66.md) |
| Q65 | What is continuous batching, and how does it differ from static batching? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_64-66.md) |
| Q66 | What is mixed precision, and why is it used during inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_64-66.md) |
| Q67 | Differentiate between online and offline LLM inference deployment scenarios and discuss their respective requirements. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_67-69.md) |
| Q68 | Explain the throughput vs latency trade-off in LLM inference. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_67-69.md) |
| Q69 | What are the various bottlenecks in a typical LLM inference pipeline when running on a modern GPU? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_67-69.md) |
| Q70 | How do you measure LLM inference performance? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_70-72.md) |
| Q71 | What are the different LLM inference engines available? Which one do you prefer? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_70-72.md) |
| Q72 | What are the challenges in LLM inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_70-72.md) |
| Q73 | What are the possible options for accelerating LLM inference? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_73-75.md) |
| Q74 | What is Chain-of-Thought prompting, and when is it useful? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_73-75.md) |
| Q75 | Explain the reason behind the effectiveness of Chain-of-Thought (CoT) prompting. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_73-75.md) |
| Q76 | Explain the trade-offs in using CoT prompting. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_76-78.md) |
| Q77 | What is prompt engineering, and why is it important for LLMs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_76-78.md) |
| Q78 | What is the difference between zero-shot and few-shot prompting? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_76-78.md) |
| Q79 | What are the different approaches for choosing examples for few-shot prompting? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_79-81.md) |
| Q80 | Why is context length important when designing prompts for LLMs? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_79-81.md) |
| Q81 | What is a system prompt, and how does it differ from a user prompt? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_79-81.md) |
| Q82 | What is In-Context Learning (ICL), and how is few-shot prompting related? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_82-84.md) |
| Q83 | What is self-consistency prompting, and how does it improve reasoning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_82-84.md) |
| Q84 | Why is context important in prompt design? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_82-84.md) |
| Q85 | Describe a strategy for reducing hallucinations via prompt design. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_85-87.md) |
| Q86 | How would you structure a prompt to ensure the LLM output is in a specific format, like JSON? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_85-87.md) |
| Q87 | Explain the purpose of ReAct prompting in AI agents. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_85-87.md) |
| Q88 | What are the different phases in LLM development? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_88-90.md) |
| Q89 | What are the different types of LLM fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_88-90.md) |
| Q90 | What role does instruction tuning play in improving an LLM‚Äôs usability? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_88-90.md) |
| Q91 | What role does alignment tuning play in improving an LLM's usability? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_91-93.md) |
| Q92 | How do you prevent overfitting during fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_91-93.md) |
| Q93 | What is catastrophic forgetting, and why is it a concern in fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_91-93.md) |
| Q94 | What are the strengths and limitations of full fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_94-96.md) |
| Q95 | Explain how parameter efficient fine-tuning addresses the limitations of full fine-tuning. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_94-96.md) |
| Q96 | When might prompt engineering be preferred over task-specific fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_94-96.md) |
| Q97 | When should you use fine-tuning vs RAG? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_97-99.md) |
| Q98 | What are the limitations of using RAG over fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_97-99.md) |
| Q99 | What are the limitations of fine-tuning compared to RAG? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_97-99.md) |
| Q100 | When should you prefer task-specific fine-tuning over prompt engineering? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_100-102.md) |
| Q101 | What is LoRA, and how does it work? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_100-102.md) |
| Q102 | Explain the key ingredient behind the effectiveness of the LoRA technique. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_100-102.md) |
| Q103 | What is QLoRA, and how does it differ from LoRA? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_103-105.md) |
| Q104 | When would you use QLoRA instead of standard LoRA? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_103-105.md) |
| Q105 | How would you handle LLM fine-tuning on consumer hardware with limited GPU memory? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_103-105.md) |
| Q106 | Explain different preference alignment methods and their trade-offs. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_106-108.md) |
| Q107 | What is gradient accumulation, and how does it help with fine-tuning large models? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_106-108.md) |
| Q108 | What are the possible options to speed up LLM fine-tuning? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_106-108.md) |
| Q109 | Explain the pretraining objective used in LLM pretraining. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_109-111.md) |
| Q110 | What is the difference between casual language modeling and masked language modeling? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_109-111.md) |
| Q111 | How do LLMs handle out-of-vocabulary (OOV) words? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_109-111.md) |
| Q112 | In the context of LLM pretraining, what is scaling law? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_112-114.md) |
| Q113 | Explain the concept of Mixture-of-Experts (MoE) architecture and its role in LLM pretraining. | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_112-114.md) |
| Q114 | What is model parallelism, and how is it used in LLM pre-training? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_112-114.md) |
| Q115 | What is the significance of self-supervised learning in LLM pretraining? | [Answer](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub/blob/main/Interview_QA/QA_115-117.md) |



