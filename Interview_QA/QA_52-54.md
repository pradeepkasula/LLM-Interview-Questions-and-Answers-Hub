Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q52: Compare deterministic and stochastic decoding methods in LLMs.

### âœ… Answer

Deterministic decoding methods, like Greedy Search and Beam Search, consistently yield the same output for the same input because they select the next token based purely on the highest probability or a fixed set of high-probability paths, respectively, without any element of randomness. 

In contrast, stochastic decoding methods, such as Temperature Sampling, Top-K Sampling, and Top-p (Nucleus) Sampling, introduce randomness to the token selection process, resulting in varied and diverse outputs even for identical inputs. 

Deterministic methods are generally preferred for tasks requiring predictability and factual accuracy (e.g., translation, summarization), while stochastic methods excel in open-ended or creative tasks (e.g., storytelling, brainstorming) where novelty is desirable.

## ğŸ“Œ Q53: What is the role of the context window during LLM inference?

### âœ… Answer

The context window refers to the maximum total number of tokens (both input and output) that the model can process at one time.  The context window during LLM inference serves as the model's working memory. This window enables the model to consider both the current input and preceding tokens to generate contextually relevant and coherent responses. 

A larger context window allows the LLM to handle longer texts or complex tasks by preserving more information. However, if the input surpasses the window size, the model "forgets" earlier tokens, potentially reducing response accuracy. â€‹Essentially, it determines how much past information the model can use to make the next prediction.

## ğŸ“Œ Q54: Explain the pros and cons of large and small context windows in LLM inference.

### âœ… Answer

The context window in LLMs defines the maximum number of tokens it can process at once, and it is like the modelâ€™s working memory. A large context window allows the model to process more input, leading to more coherent and contextually aware outputs. However, processing more input tokens significantly increases memory usage, computational cost, and latency. 

Conversely, a small context window is faster and more memory-efficient, making it suitable for lower-resource environments or real-time applications. However,  it limits the model's 'memory,' potentially causing it to lose track of earlier information and generate less consistent or relevant outputs. 


## ğŸ‘¨ğŸ»â€ğŸ’» LLM Survey Papers Collection

ğŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------





