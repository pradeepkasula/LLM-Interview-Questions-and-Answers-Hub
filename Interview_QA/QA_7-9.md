Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## üìå Q7: Explain why subword tokenization is preferred over word-level tokenization in the Transformer model. 

### ‚úÖ Answer

Subword tokenization, such as Byte-Pair Encoding (BPE) in the Transformer model, offers several advantages over word-level tokenization by handling out-of-vocabulary words and balancing vocabulary size. It effectively handles out-of-vocabulary (OOV) words (unseen words, i.e., words not in the model vocabulary) by breaking them into known subwords, significantly improving coverage of rare or unseen words. 

This method also results in a smaller, manageable vocabulary size compared to the vast lexicon of word-level tokenization, while still providing the semantic benefits of word-level units. Finally, it helps the model learn about morphology (prefixes, suffixes) by segmenting words into meaningful parts.

## üìå Q8: Explain the trade-offs in using a large vocabulary in LLMs.

### ‚úÖ Answer

Using a large vocabulary in LLMs allows the model to represent words more precisely and reduce token fragmentation, improving understanding of rare or domain-specific terms. However, it increases the size of the embedding and output layers, leading to higher memory usage and slower training. 

Larger vocabularies also make softmax computations more expensive and may lead to a sparser representation of less frequent tokens, potentially hindering effective learning for those words. Therefore, models often moderate vocabulary sizes to ensure a balance between linguistic coverage and computational efficiency. 

## üìå Q9: Explain how self-attention is computed in the Transformer model step by step. 

### ‚úÖ Answer

Self-attention in the Transformer model is computed by first projecting each input token into three vectors: queries, keys, and values using learned weight matrices. Then, attention scores are calculated by taking the dot product of a query with all keys, scaled by the square root of the key dimension to stabilize gradients. 

These scores are normalized using the softmax function to obtain attention weights, representing the importance of each token relative to others. Finally, each token's output is computed as the weighted sum of the value vectors, allowing the model to capture contextual relationships within the input sequence.

## **üë®üèª‚Äçüíª Prompt Engineering Techniques Hub**

This GitHub repo includes implementations of must know 25+ prompt engineering techniques.

üëâ [Repo link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub)

Knowledge of prompt engineering techniques is essential for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents. 

![Prompt Engineering Techniques Hub](images/prompt-eng-techniques-hub.jpg)
 

------------------------------------------------------------------------------------------






