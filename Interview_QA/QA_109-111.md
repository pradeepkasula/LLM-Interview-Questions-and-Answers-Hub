Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q109: Explain the pretraining objective used in LLM pretraining.

### âœ… Answer

The pre-training objective of large language models is next token prediction, also known as causal language modeling. In this setup, the model learns to predict the next token in a sequence given all the previous ones. By minimizing the difference between its predictions and the actual next tokens across billions of examples, the model gradually learns grammar, semantics, and contextual relationships. 

This objective enables LLMs to generate coherent, contextually relevant text and perform a wide range of downstream tasks through prompting or fine-tuning.

## ğŸ“Œ Q110: What is the difference between casual language modeling and masked language modeling?

### âœ… Answer

Causal Language Modeling (CLM) is an autoregressive approach  where the model predicts the next token in a sequence based only on the preceding tokens. 

In contrast, Masked Language Modeling (MLM) is an autoencoding approach where the model predicts intentionally masked (missing) tokens by leveraging bidirectional context, i.e., it considers both the past and future tokens in the sequence. 

## ğŸ“Œ Q111: How do LLMs handle out-of-vocabulary (OOV) words?

### âœ… Answer

LLMs handle out-of-vocabulary (OOV) words using subword tokenization methods such as Byte Pair Encoding (BPE), WordPiece, or SentencePiece. These techniques split rare or unseen words into smaller, known subword units or characters. 

This allows the model to represent and understand new words from existing tokens in the vocabulary. For example, the word â€œunhappinessâ€ might be split into â€œunâ€, â€œhappyâ€, and â€œnessâ€. This approach reduces the OOV problem and improves generalization to unseen vocabulary.

## **ğŸ‘¨ğŸ»â€ğŸ’» Prompt Engineering Techniques Hub**

This GitHub repo includes implementations of must know 25+ prompt engineering techniques.

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub)

Knowledge of prompt engineering techniques is essential for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents. 

![Prompt Engineering Techniques Hub](images/prompt-eng-techniques-hub.jpg)
 

------------------------------------------------------------------------------------------





