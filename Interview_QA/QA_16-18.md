Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ðŸ“Œ Q16: What is the purpose of the encoder in a transformer model?

### âœ… Answer

The purpose of the encoder in a transformer model is to process the entire input sequence and generate context-aware numerical representations. The encoder uses multi-head self-attention and feed-forward neural networks to capture the relationships and dependencies between all the tokens to generate context-rich token representations. These context-rich token representations are then passed to the decoder, which generates the output sequence.


## ðŸ“Œ Q17: What is the purpose of the decoder in a transformer model?

### âœ… Answer

The purpose of the decoder in a transformer model is to generate the output sequence based on the contextual representations provided by the encoder and the previously generated tokens. 

The decoder layers use (i) masked self-attention to ensure that predictions for the current step rely solely on previously generated outputs and (ii) encoder-decoder attention to focus on the most relevant parts of the encoder's final output when generating each output token.  

In simple words,  the decoder transforms the encoderâ€™s output (contextual representations) into the desired output token by token.

## ðŸ“Œ Q18: How does the encoder-decoder structure work at a high level in the Transformer model?

### âœ… Answer

The encoder-decoder structure in a Transformer is used for sequence-to-sequence tasks like machine translation. At a high level, the encoder processes the entire input sequence simultaneously, using self-attention to create rich, context-aware vector representations (the encoded state). 

The decoder then uses a separate self-attention mechanism, masked to prevent looking ahead, to generate the output sequence one token at a time, autoregressively. The decoder also incorporates an encoder-decoder attention layer, allowing it to focus on the most relevant parts of the encoder's output for each token it generates.

## LLM Survey Papers Collection

ðŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------




