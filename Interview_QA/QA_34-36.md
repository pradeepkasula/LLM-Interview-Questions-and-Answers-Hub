Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q34: How does Transformer model address the vanishing gradient problem?

### âœ… Answer

Transformers address the vanishing gradient problem primarily through the use of residual (skip) connections and layer normalization. Residual connections allow gradients to flow directly through the network by adding the input of a layer to its output, preventing gradients from shrinking during backpropagation.

Layer normalization helps stabilize training by keeping activations within a consistent range, which controls the scale of gradients. Additionally, the self-attention mechanism enables direct information flow between tokens, reducing the depth of dependency and further mitigating gradient vanishing.

## ğŸ“Œ Q35: What is the purpose of the position-wise feed-forward sublayer in the Transformer model?

### âœ… Answer

The position-wise feed-forward sublayer in the Transformer model serves to independently process each position's representation in the sequence after the attention mechanism. It consists of two linear transformations with a ReLU activation in between, applied identically and separately to each token vector. 

This sublayer adds non-linearity and increases the model's capacity to learn complex features by transforming the attention output into richer representations for the next layers.

## ğŸ“Œ Q36: Can you briefly explain the difference between LLM training and LLM inference?

### âœ… Answer

LLM training is the process where a large language model learns patterns and relationships from massive datasets, adjusting its internal parameters (weights) to minimize prediction error. LLM inference is the deployment phase where the trained, fixed-parameter model uses its learned knowledge to generate a response for a new, unseen input prompt. Training is computationally intensive and done once, while inference is fast and happens every time the model is used.

**â˜• Support the Author**
-------------------------------------------------------------------------------------------
I hope you found this â€œLLM Interview Questions and Answers Hubâ€  highly useful.  

Iâ€™ve made this freely available to help the AI and NLP community grow and to support learners like you. If you found it helpful and would like to show your appreciation, you can buy me a coffee to keep me motivated in creating more free resources like this.

ğŸ‘‰ [Buy Me a Coffee](https://ko-fi.com/kalyanksnlp)

Your small gesture goes a long way in supporting my workâ€”thank you for being part of this journey! ğŸ™

â€” Kalyan KS

---------------------------------------------------------------------------------------------




