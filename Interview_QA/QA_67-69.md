Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ðŸ“Œ Q67: Differentiate between online and offline LLM inference deployment scenarios and discuss their respective requirements.

### âœ… Answer

Online LLM inference involves real-time, user-facing requests typically hosted on a cloud server, requiring low latency and high throughput to handle unpredictable traffic and network communication efficiently. 

Conversely, offline LLM inference deals with precollected data in batches, usually on on-premise or local hardware, where the primary requirements are high throughput and processing large data volumes at scale, with less stringent latency demands. The online scenario prioritizes rapid individual responses, while the offline scenario focuses on massive-scale, non-real-time data processing.

## ðŸ“Œ Q68: Explain the throughput vs. latency tradeoff in LLM inference.

### âœ… Answer

Latency refers to how long it takes to process a single request -  the faster it responds, the lower the latency. This is usually achieved by handling smaller batches of data at a time. On the other hand, throughput measures how many requests a system can handle per second, which improves when larger batches are processed together to fully utilize the GPU. 

However, doing so makes individual requests wait longer, increasing latency. Hence, systems must balance these metrics based on their application - interactive applications like chatbots focus on low latency for quick responses, while batch-processing systems aim for high throughput to maximize efficiency.

## ðŸ“Œ Q69: What are the various bottlenecks in a typical LLM inference pipeline when running on a modern GPU?

### âœ… Answer

When running large language models (LLMs) on modern GPUs, several key bottlenecks limit performance. One major issue is memory bandwidth saturation, where the model frequently accesses large key-value (KV) caches, slowing data movement. 

As the KV cache grows during text generation, it consumes more GPU memory, forcing smaller batch sizes and creating memory pressure. Compute bottlenecks also occur in heavy operations like matrix multiplications, though these are often less critical than memory-related delays. 

In hybrid CPU-GPU systems, inefficient task scheduling can leave GPU cores underutilized, while multi-GPU setups face extra communication delays that reduce scalability. Overcoming these challenges involves techniques like caching, model quantization, smarter scheduling, and balanced workload distribution to fully harness GPU power and minimize latency.


## LLM Survey Papers Collection

ðŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------




