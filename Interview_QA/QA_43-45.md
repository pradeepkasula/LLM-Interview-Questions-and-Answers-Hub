Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ðŸ“Œ Q43: What are the different decoding strategies in LLMs?

### âœ… Answer

Decoding strategies in LLMs determine how tokens are selected during text generation. The main strategies are:

- Greedy Decoding - Always picks the token with the highest probability at each step. Itâ€™s fast but can produce repetitive or suboptimal text.

- Beam Search - Keeps multiple best candidate sequences (â€œbeamsâ€) at each step and chooses the most likely overall sequence. It improves quality but increases computation.

- Top-k Sampling - Randomly samples the next token from the top k most probable tokens, adding diversity and reducing repetition.

- Top-p (Nucleus) Sampling - Randomly samples from the smallest set of tokens whose cumulative probability â‰¥ p (e.g., 0.9). 

- Temperature Sampling - Adjusts randomness by scaling logits before softmax â€” higher temperature (>1) makes outputs more random; lower (<1) makes them more deterministic.

- Speculative Decoding - Uses a smaller draft model to predict multiple tokens ahead and quickly verifies them with the main model, greatly speeding up generation without losing quality. 

## ðŸ“Œ Q44: Explain the impact of the decoding strategy on LLM-generated output quality and latency.

### âœ… Answer

Decoding strategies critically impact LLM output by balancing quality (coherence, diversity, and relevance) and latency (generation speed). Deterministic methods like Greedy Search are fast due to selecting the highest probability token at each step but often yield repetitive, lower-quality text.  Beam Search improves quality by exploring multiple token sequences but increases latency due to managing several beams. 

Stochastic decoding methods, such as Top-k or Top-p (nucleus) sampling, generate text token by token, choosing each next token probabilistically rather than deterministically. While this increases creativity and diversity, it also introduces extra computational overhead (sorting  or cumulative sum) at every decoding step, which increases latency.

Newer, faster techniques like Speculative Decoding reduce latency by using a smaller model to draft tokens, which the larger model verifies in parallel, offering significant speedups while aiming to preserve the high output quality of the original model.

## ðŸ“Œ Q45: Explain the greedy search decoding strategy and its main drawback.

### âœ… Answer

The greedy search decoding strategy in LLMs is a straightforward method where the model selects the token with the highest probability as the next word at each step of the generation process. This approach is highly efficient and deterministic, always producing the same output for a given input. Its primary drawback, however, is its shortsightedness. 

By only focusing on the locally optimal choice, it often fails to find a sequence with the globally highest overall probability, potentially leading to repetitive, less coherent, or suboptimal text.

## **ðŸš€ AIxFunda Newsletter (free)**


Join ðŸš€ AIxFunda free newsletter to get the latest updates and interesting tutorials related to Generative AI, LLMs, Agents and RAG.

- âœ¨ Weekly GenAI updates.
- ðŸ“„ Weekly LLM, Agents and RAG paper updates.
- ðŸ“ 1 fresh blog post on an interesting topic every week.

ðŸ‘‰ [Subcribe Now](https://aixfunda.substack.com/) 

---------------------------------------------------------------------------------------------






