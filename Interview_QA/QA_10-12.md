Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## üìå Q10: What is the computational complexity of self-attention in the Transformer model?

### ‚úÖ Answer

The computational complexity of the self-attention mechanism in the Transformer model is primarily quadratic with respect to the sequence length $n$. Formally, it is O($n^2$.d). The O($n^2$.d) comes from computing the attention scores between all pairs of tokens.  Since there are $n$ x $n$ = $n^2$ pairwise interactions, and each involves a dot product of d dimensional vectors, this results in $n^2$.d operations.  

The quadratic complexity with respect to sequence length $n$  makes self-attention computationally expensive for long sequences.  However, this design allows the model to capture relationships between all tokens simultaneously, which is crucial for its performance. Despite its quadratic complexity, it remains faster than traditional RNNs in practice due to parallelization benefits.


## üìå Q11: How do Transformer model address the vanishing gradient problem?

### ‚úÖ Answer

Transformers address the vanishing gradient problem primarily through the use of residual (skip) connections and layer normalization. Residual connections allow gradients to flow directly through the network by adding the input of a layer to its output, preventing gradients from shrinking during backpropagation. 

Layer normalization helps stabilize training by keeping activations within a consistent range, which controls the scale of gradients. Additionally, the self-attention mechanism enables direct information flow between tokens, reducing the depth of dependency and further mitigating gradient vanishing.

## üìå Q12: What is tokenization, and why is it necessary in LLMs?

### ‚úÖ Answer

Tokenization is the process of converting raw text into smaller, discrete units called tokens, which can be words, sub-words, or characters. This step is necessary in LLMs because models cannot directly process raw strings of text; they require numerical input. 

Tokenization allows mapping the input text to a vocabulary of known tokens. These tokens are then converted into numerical representations called embeddings. These embeddings are then used by LLMs for training and inference, fundamentally enabling input text processing.


**‚òï Support the Author**
-------------------------------------------------------------------------------------------
I hope you found this ‚ÄúLLM Interview Questions and Answers Hub‚Äù  highly useful.  

I‚Äôve made this freely available to help the AI and NLP community grow and to support learners like you. If you found it helpful and would like to show your appreciation, you can buy me a coffee to keep me motivated in creating more free resources like this.

üëâ [Buy Me a Coffee](https://ko-fi.com/kalyanksnlp)

Your small gesture goes a long way in supporting my work‚Äîthank you for being part of this journey! üôè

‚Äî Kalyan KS

---------------------------------------------------------------------------------------------






