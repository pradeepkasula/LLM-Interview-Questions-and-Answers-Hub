Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q1: CNNs and RNNs donâ€™t use positional embeddings. Why do transformers use positional embeddings?

### âœ… Answer

CNNs and RNNs donâ€™t use positional embeddings because these models inherently capture positional information through convolutional filters or recurrence over time steps. However, the Transformer model uses the self-attention mechanism, which processes all tokens in parallel without any notion of sequence or order. 

Positional embeddings inject explicit information about each tokenâ€™s position in the sequence, allowing transformers to understand word order and relative distances between tokens. This helps them model sequential dependencies and sentence structure effectively, compensating for the parallel and order-agnostic nature of self-attention mechanisms.

## ğŸ“Œ Q2: Tell me the basic steps involved in running an inference query on an LLM.

### âœ… Answer

Running an inference query on an LLM involves the following steps:

- Tokenization: The input text is first broken down into tokens, and then the tokens are mapped to their IDs in the modelâ€™s vocabulary. 

- Prefill Phase: The embedding layer converts these token IDs into embeddings, and then the decoder layers process these embeddings simultaneously to compute intermediate states (keys and values). This parallel processing establishes the context for generating new tokens.

- Decoding Phase: The model generates output tokens one at a time autoregressively, each based on previously generated tokens and cached states, continuing until a stopping condition is met.

- Detokenization: Finally, the generated tokens are converted back into human-readable text.

## ğŸ“Œ Q3: Explain how KV Cache accelerates LLM inference.

### âœ… Answer

KV Cache speeds up LLM inference by storing the attention key (K) and value (V) representations that were computed for previous tokens. This allows the model to reuse them instead of recalculating them at each decoding step. During text generation, only the query for the latest token is computed and then combined with the cached K and V vectors to produce the next token, significantly reducing redundant computation. 

This dramatically speeds up inferenceâ€”often delivering several-fold faster token generation. However, the cache consumes substantial GPU memory, which is why techniques like KV cache offloading and compression are used to balance speed and memory efficiency in large-scale LLM serving.

## **ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**

ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents.

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)
 

---------------------------------------------------------------------------------------------




