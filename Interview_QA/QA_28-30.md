Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q28: What is the purpose of residual (skip) connections in Transformer model layers?

### âœ… Answer

The main purpose of residual (skip) connections in Transformer layers is to allow the gradients to flow directly through the network during backpropagation. This helps to mitigate the vanishing gradient problem in deep networks, ensuring effective training. 

Specifically, they enable the output of a layer to be Layer Input + Layer Output rather than just the Layer Output. This means the layer learns an additive change rather than a new representation entirely. This facilitates training very deep architectures, leading to better overall performance.

## ğŸ“Œ Q29: Why is layer normalization used, and where is it applied in Transformer?

### âœ… Answer

Layer normalization is used in Transformer to stabilize and accelerate training by normalizing the inputs across features within each layer, ensuring that activations have a mean of zero and variance of one. This normalization helps control the scale of gradients during backpropagation, preventing issues like exploding or vanishing gradients. 

In Transformer architectures, layer normalization is applied before the self-attention and feedforward sublayers (Pre-LN) or after these sublayers within the residual connections (Post-LN). The Pre-LN variant improves gradient stability and allows faster training without requiring learning rate warm-up, while Post-LN was used in the original Transformer but can lead to unstable gradients if not carefully managed.

## ğŸ“Œ Q30: What is cross-entropy loss, and how is it applied during transformer training?

### âœ… Answer

Cross-entropy loss measures the difference between the predicted probability distribution of a model and the true distribution of target labels. In Transformer training, it quantifies how well the model predicts the next token by comparing its output logits (after softmax) with the actual token index. 

The loss penalizes incorrect predictions with higher values, encouraging the model to assign greater probability to the correct token. Minimizing the cross-entropy loss through backpropagation helps optimize the modelâ€™s parameters for accurate sequence generation.

## **ğŸ‘¨ğŸ»â€ğŸ’» Prompt Engineering Techniques Hub**

This GitHub repo includes implementations of must know 25+ prompt engineering techniques.

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub)

Knowledge of prompt engineering techniques is essential for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents. 

![Prompt Engineering Techniques Hub](images/prompt-eng-techniques-hub.jpg)
 

------------------------------------------------------------------------------------------





