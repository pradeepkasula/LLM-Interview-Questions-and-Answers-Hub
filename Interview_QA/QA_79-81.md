Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q79: What are the different approaches for choosing examples for few-shot prompting?

### âœ… Answer

Proper choice of examples in few-shot prompting aims to maximize the informativeness and relevance of the limited context provided to the LLM. One common approach is random sampling, where examples are chosen arbitrarily from the dataset to provide a general overview. 

Diversity-based selection ensures the examples cover a wide range of input scenarios and edge cases relevant to the task, preventing overfitting. Conversely, similarity-based selection (often using vector embeddings) retrieves examples semantically most similar to the input query, providing the most direct in-context guidance. 

## ğŸ“Œ Q80: Why is context length important when designing prompts for LLMs?

### âœ… Answer

Context length is crucial because it dictates the maximum number of tokens an LLM can consider when generating a response. If the prompt having instructions, examples, and input data exceeds this limit, the model will truncate the input. 

This leads to a loss of vital information, which results in incomplete, irrelevant, or inaccurate outputs. Therefore, understanding and managing the context length is essential for designing concise yet comprehensive prompts that fully leverage the model's capabilities.

## ğŸ“Œ Q81: What is a system prompt, and how does it differ from a user prompt?

### âœ… Answer

A system prompt is an instruction given to an LLM to define its overall behavior, role, or response style throughout a conversation, such as â€œYou are an expert data scientist.â€  It sets the foundation for how the model interprets and responds to inputs. 

In contrast, a user prompt is a direct query or task provided by the user during interaction, like â€œExplain the concept of embeddings.â€ In simple words,  the system prompt defines the modelâ€™s persona and boundaries, while the user prompt provides the query to be answered or the task to be performed.


## **ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**

ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents.

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)
 

---------------------------------------------------------------------------------------------





