Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q22: How does masked self-attention differ from regular self-attention, and where is it used in a Transformer?

### âœ… Answer

Masked self-attention differs from regular self-attention by restricting the attention mechanism so that each position in the sequence can only attend to earlier positions and itself, preventing access to future tokens. This is essential for autoregressive tasks like text generation, where tokens are generated sequentially one by one. 

Regular self-attention allows every token to attend to all tokens in the sequence, which is useful in encoder layers for understanding context bidirectionally. Masked self-attention is specifically used in the decoder layers of the Transformer model to ensure that the model predicts tokens one at a time without peeking ahead.

## ğŸ“Œ Q23: Discuss the pros and cons of the self-attention mechanism in the Transformer model.

### âœ… Answer

Self-attention enables the Transformer model to capture long-range dependencies (contextual relationships) between tokens in the input sequence efficiently. It allows for parallel computation, improving training speed compared to recurrent models. 

However, its quadratic time complexity with respect to sequence length and large memory footprint make it computationally expensive for long inputs.

## ğŸ“Œ Q24: What is the purpose of masked self-attention in the Transformer decoder?

### âœ… Answer

During output generation, masked self-attention in the decoder prevents the model from looking ahead by blocking access to future tokens during generation. This means that when predicting a token, the model is restricted to attending only to the tokens that come before it in the sequence. 

This ensures (i) auto-regressive behavior, allowing the model to generate text one token at a time, and (ii) the model learns to make predictions based solely on past context, maintaining the integrity and correctness of language modeling.

**â˜• Support the Author**
-------------------------------------------------------------------------------------------
I hope you found this â€œLLM Interview Questions and Answers Hubâ€  highly useful.  

Iâ€™ve made this freely available to help the AI and NLP community grow and to support learners like you. If you found it helpful and would like to show your appreciation, you can buy me a coffee to keep me motivated in creating more free resources like this.

ğŸ‘‰ [Buy Me a Coffee](https://ko-fi.com/kalyanksnlp)

Your small gesture goes a long way in supporting my workâ€”thank you for being part of this journey! ğŸ™

â€” Kalyan KS

---------------------------------------------------------------------------------------------





